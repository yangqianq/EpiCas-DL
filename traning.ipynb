{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e87392",
   "metadata": {},
   "source": [
    "# model training with CRISPRoff dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061893fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from function import *\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c74c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = pd.read_csv(\"./dataset/testfile.csv\")\n",
    "data = pd.DataFrame(columns=(['40mer']))\n",
    "data['40mer'] = FILE['40mer']\n",
    "\n",
    "x_data = data.iloc[:, 0]\n",
    "x_data = grna_preprocess(x_data,40)\n",
    "\n",
    "y_data = FILE['efficiency']\n",
    "y_data = np.array(y_data)\n",
    "y_data = y_data.reshape(len(y_data), -1)\n",
    "\n",
    "tss1,tss2,tss3,tss4 = FILE['nor_tss1'],FILE['nor_tss2'],FILE['nor_tss3'],FILE['nor_tss4']\n",
    "tss1,tss2,tss3,tss4 = epi_progress(tss1),epi_progress(tss2),epi_progress(tss3),epi_progress(tss4)\n",
    "Methylation,ATAC,RNA = FILE['nor_methylation'],FILE['nor_atac'],FILE['nor_rna']\n",
    "Methylation,ATAC,RNA = epi_progress(Methylation),epi_progress(ATAC),epi_progress(RNA)\n",
    "model_input = np.concatenate((x_data, tss1, tss2, tss3, tss4, Methylation, ATAC, RNA), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798068c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(model_input, y_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2efedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    data1_input = Input(shape=(1, 40, 11))\n",
    "    data1_Conv1 = Conv2D(filters=30, kernel_size=(1, 1), padding='same', activation='relu')(data1_input)\n",
    "    data1_Conv2 = Conv2D(filters=30, kernel_size=(1, 2), padding='same', activation='relu')(data1_input)\n",
    "    data1_Conv3 = Conv2D(filters=30, kernel_size=(1, 3), padding='same', activation='relu')(data1_input)\n",
    "    data1_Conv4 = Conv2D(filters=30, kernel_size=(1, 4), padding='same', activation='relu')(data1_input)\n",
    "    data1_Conv5 = Conv2D(filters=30, kernel_size=(1, 5), padding='same', activation='relu')(data1_input)\n",
    "    data1_t = tf.keras.layers.Concatenate()([data1_Conv1, data1_Conv2, data1_Conv3, data1_Conv4, data1_Conv5])\n",
    "    #data1_conv1_2 = Conv2D(filters=120, strides=(1, 2), kernel_size=(1, 3), padding='valid')(data1_t)\n",
    "    data1_p1 = MaxPool2D(strides=2, padding='same')(data1_t)\n",
    "    #BN1 = BatchNormalization()(data1_p1)\n",
    "    #data1_d1 = Dropout(0.5)(data1_p1)\n",
    "\n",
    "    flatten = Flatten()(data1_p1)\n",
    "    BN1 = BatchNormalization()(flatten)\n",
    "    #f5 = Dense(100, activation='relu')(flatten)\n",
    "    f3 = Dense(80, activation='relu')(BN1)\n",
    "    BN2 = BatchNormalization()(f3)\n",
    "    drop1 = Dropout(0.5)(BN2)\n",
    "    f6 = Dense(60, activation='relu')(drop1)\n",
    "    BN3 = BatchNormalization()(f6)\n",
    "    drop2 = Dropout(0.5)(BN3)\n",
    "    f4 = Dense(40, activation='relu')(drop2)\n",
    "    BN4 = BatchNormalization()(f4)\n",
    "    #f7 = Dense(40, activation='relu')(f4)\n",
    "    #BN2 = BatchNormalization()(f4)\n",
    "    d2 = Dropout(0.5)(BN4)\n",
    "    output = Dense(1, activation=\"linear\", name=\"output\")(d2)\n",
    "    model = Model(inputs=data1_input, outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e42202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    data_input = Input(shape=(1, 40, 11))\n",
    "    data_Conv1 = Conv2D(filters=30, kernel_size=(1, 1), padding='same', activation='relu')(data_input)\n",
    "    data_Conv2 = Conv2D(filters=30, kernel_size=(1, 2), padding='same', activation='relu')(data_input)\n",
    "    data_Conv3 = Conv2D(filters=30, kernel_size=(1, 3), padding='same', activation='relu')(data_input)\n",
    "    data_Conv4 = Conv2D(filters=30, kernel_size=(1, 4), padding='same', activation='relu')(data_input)\n",
    "    data_Conv5 = Conv2D(filters=30, kernel_size=(1, 5), padding='same', activation='relu')(data_input)\n",
    "    data_t = tf.keras.layers.Concatenate()([data_Conv1, data_Conv2, data_Conv3, data_Conv4, data_Conv5])\n",
    "    data_p = MaxPool2D(strides=2, padding='same')(data_t)\n",
    "    data_d1 = Dropout(0.4)(data_p)\n",
    "\n",
    "    flatten = Flatten()(data_d1)\n",
    "    BN1 = BatchNormalization()(flatten)\n",
    "    f1 = Dense(80, activation='relu')(BN1)\n",
    "    BN2 = BatchNormalization()(f1)\n",
    "    drop1 = Dropout(0.4)(BN2)\n",
    "    f2 = Dense(60, activation='relu')(drop1)\n",
    "    BN3 = BatchNormalization()(f2)\n",
    "    drop2 = Dropout(0.4)(BN3)\n",
    "    f3 = Dense(40, activation='relu')(drop2)\n",
    "    BN4 = BatchNormalization()(f3)\n",
    "    drop3 = Dropout(0.4)(BN4)\n",
    "    output = Dense(1, activation=\"linear\", name=\"output\")(drop3)\n",
    "    model = Model(inputs=data_input, outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_build()\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "                loss='mse'    ,\n",
    "                metrics=['mse'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "spermanr = sp.stats.spearmanr(y_test, y_test_pred)[0]\n",
    "print(spermanrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8efd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/CRISPRoff_seq_sper.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba12f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(loss, label='loss')\n",
    "plt.plot(val_loss, label='val_loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0585804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xy] *",
   "language": "python",
   "name": "conda-env-xy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
